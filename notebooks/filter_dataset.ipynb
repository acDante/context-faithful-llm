{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter NQ dev data by only keeping the instances that the model can give correct original answers\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import jsonlines\n",
    "import string\n",
    "import re\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of original examples:  2773\n",
      "Number of counterfactual examples:  2773\n"
     ]
    }
   ],
   "source": [
    "# Filter the NQ test data provided in the NQ repository for Llama2 experiments\n",
    "# Here, the author only filters the test data provided in the git repo by\n",
    "# keeping a subset of data points for which the model can generate the correct original answer in its output\n",
    "\n",
    "proj_path = Path(\"/home/xiaotang/Project/context-faithful-llm/\")\n",
    "data_path = proj_path / \"datasets\"\n",
    "orig_data_path = data_path / \"nq\" / \"orig_dev_filtered.json\"\n",
    "sub_data_path = data_path / \"nq\" / \"conflict_dev_filtered.json\"\n",
    "\n",
    "with open(orig_data_path, \"r\") as fin:\n",
    "    orig_examples = json.load(fin)\n",
    "\n",
    "with open(sub_data_path, \"r\") as fin:\n",
    "    counter_examples = json.load(fin)\n",
    "\n",
    "print(\"Number of original examples: \", len(orig_examples))\n",
    "print(\"Number of counterfactual examples: \", len(counter_examples))\n",
    "assert(len(orig_examples) == len(counter_examples))\n",
    "\n",
    "# Load predictions of Llama2-chat models\n",
    "pred_path = proj_path / \"results\" / \"llama2-7b-chat-no-demo-closed-book_preds.json\"\n",
    "predictions = []\n",
    "with open(pred_path, \"r\") as fin:\n",
    "    predictions = json.load(fin)\n",
    "\n",
    "assert(len(predictions) == len(orig_examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1610\n",
      "0.5805986296429859\n"
     ]
    }
   ],
   "source": [
    "# Iterate prediction file, extract a subset of examples (sample indices) for which \n",
    "# the model output contains the original answer\n",
    " \n",
    "filtered_data_indices = []\n",
    "for idx, sample in enumerate(predictions):\n",
    "    golds = sample['answer']\n",
    "    pred = sample['prediction']\n",
    "\n",
    "    recall = 0\n",
    "    if isinstance(golds, list):\n",
    "        for g in golds:\n",
    "            recall = max(recall_score(pred, g), recall)\n",
    "    else:\n",
    "        recall = recall_score(pred, golds)\n",
    "    \n",
    "    if  recall == True:\n",
    "        filtered_data_indices.append(idx)\n",
    "\n",
    "print(len(filtered_data_indices))\n",
    "print(len(filtered_data_indices) / len(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1610\n",
      "1610\n"
     ]
    }
   ],
   "source": [
    "# Filter the original and couterfactual samples for Llama2 experiments\n",
    "\n",
    "filtered_orig_samples = []\n",
    "filtered_counter_samples = []\n",
    "\n",
    "for idx in filtered_data_indices:\n",
    "    filtered_orig_samples.append(orig_examples[idx])\n",
    "    filtered_counter_samples.append(counter_examples[idx])\n",
    "\n",
    "print(len(filtered_orig_samples))\n",
    "print(len(filtered_counter_samples))\n",
    "\n",
    "# Write filtered NQ test data (for Llama-2 experiments) to file\n",
    "\n",
    "output_path = data_path / \"nq_llama2\"\n",
    "with open(output_path / \"orig_dev_filtered.json\", 'w') as fout:\n",
    "    json.dump(filtered_orig_samples, fout, indent=4)\n",
    "\n",
    "with open(output_path / \"conflict_dev_filtered.json\", 'w') as fout:\n",
    "    json.dump(filtered_counter_samples, fout, indent=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â Load NQ dev data\n",
    "proj_path = Path('/home/xiaotang/Project/entity_perturb')\n",
    "data_path = proj_path / \"data\"\n",
    "orig_data_path = data_path /  \"MRQANaturalQuestionsDev_orig.jsonl\"\n",
    "orig_answers = get_gold_answers(orig_data_path)\n",
    "\n",
    "# Load the predictions of Llama2-chat models\n",
    "pred_path = proj_path / \"results\" / \"llama2-7b-chat-nq-origin-greedy-no_special_token_preds.json\"\n",
    "predictions = []\n",
    "with open(pred_path, 'r') as fin:\n",
    "    predictions = json.load(fin)\n",
    "\n",
    "pred_answers = []\n",
    "for pred in predictions:\n",
    "    pred_answers.append(pred['prediction'])\n",
    "\n",
    "assert(len(pred_answers) == len(orig_answers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EM 0.48461862519043697\n",
      "recall 39.633375390574436\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the exact match and recall\n",
    "\n",
    "em, recall = get_score(pred_answers, orig_answers)\n",
    "print(\"EM\", em)\n",
    "print(\"recall\", recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1881\n",
      "0.3963337547408344\n"
     ]
    }
   ],
   "source": [
    "# Extract a subset of test instances whose recall is 1\n",
    "\n",
    "filtered_data = []\n",
    "for sample, gold in zip(predictions, orig_answers):\n",
    "    pred = sample['prediction']\n",
    "    recall = 0\n",
    "    if isinstance(gold, list):\n",
    "        for g in gold:\n",
    "            recall = max(recall_score(pred, g), recall)\n",
    "    else:\n",
    "        recall = recall_score(pred, gold)\n",
    "    if recall == True:\n",
    "        filtered_data.append(sample)\n",
    "\n",
    "print(len(filtered_data))\n",
    "print(len(filtered_data) / len(orig_answers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write filtered NQ dev data to file\n",
    "# Current setting: without special tokens + greedy decoding\n",
    "\n",
    "output_path = data_path /  \"filterd_MRQANaturalQuestionsDev_orig.jsonl\"\n",
    "with jsonlines.open(output_path, mode='w') as writer:\n",
    "    writer.write_all(filtered_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1881\n"
     ]
    }
   ],
   "source": [
    "# Convert the filtered test data and its perturbed version to context faithful llm format\n",
    "\n",
    "orig_data_path = data_path / \"filterd_MRQANaturalQuestionsDev_orig.jsonl\"\n",
    "conflict_data_path = data_path / \"conflict_MRQANaturalQuestionsDev-filtered.jsonl\"\n",
    "\n",
    "converted_data = []\n",
    "src_data_path = conflict_data_path\n",
    "with jsonlines.open(src_data_path) as reader:\n",
    "    for obj in reader:\n",
    "        if \"uid\" in obj.keys():\n",
    "            data_sample = dict()\n",
    "            data_sample['question'] = obj['query']\n",
    "            data_sample['context'] = obj['context']\n",
    "            \n",
    "            answers = []\n",
    "            for ans in obj['gold_answers']:\n",
    "                answers.append(ans['text'])\n",
    "            data_sample['answer'] = answers\n",
    "            \n",
    "            converted_data.append(data_sample)\n",
    "\n",
    "print(len(converted_data))\n",
    "\n",
    "# Save the converted version of data to JSON\n",
    "with open(data_path / \"llama2_conflict_dev_filtered.json\", 'w') as fout:\n",
    "    json.dump(converted_data, fout, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_answer(s):\n",
    "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "def exact_match_score(prediction, ground_truth):\n",
    "    return (normalize_answer(prediction) == normalize_answer(ground_truth))    \n",
    "\n",
    "def recall_score(prediction, ground_truth):\n",
    "    prediction = normalize_answer(prediction)\n",
    "    ground_truth = normalize_answer(ground_truth)\n",
    "    return (ground_truth in prediction)\n",
    "\n",
    "def get_gold_answers(data_path):\n",
    "    gold_answers = []\n",
    "    with jsonlines.open(data_path) as reader:\n",
    "        for obj in reader:\n",
    "            if \"uid\" in obj.keys():\n",
    "                answers = []\n",
    "                for ans in obj['gold_answers']:\n",
    "                    answers.append(ans['text'])\n",
    "                \n",
    "                gold_answers.append(answers)\n",
    "    \n",
    "    return gold_answers\n",
    "\n",
    "def get_score(preds, golds):\n",
    "    em, recall = 0, 0\n",
    "    for pred, gold in zip(preds, golds):\n",
    "        if isinstance(gold, list):\n",
    "            _em, _recall = 0, 0\n",
    "            for g in gold:\n",
    "                _em = max(exact_match_score(pred, g), _em)\n",
    "                _recall = max(recall_score(pred, g), _recall)\n",
    "            em += _em\n",
    "            recall += _recall\n",
    "        else:\n",
    "            em += exact_match_score(pred, gold)\n",
    "            recall += recall_score(pred, gold)\n",
    "    em = em * 100 / (len(preds) + 1e-5)\n",
    "    recall = recall * 100 / (len(preds) + 1e-5)\n",
    "    return em, recall"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
